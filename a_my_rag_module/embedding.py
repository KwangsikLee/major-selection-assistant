from pathlib import Path
from typing import List, Dict, Any, Optional
import datetime
import json
import os
import pickle
# LangChain imports
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.schema import Document
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

from sentence_transformers import SentenceTransformer

# =============================================================================
# ë‹¤ì¤‘ ì„ë² ë”© ë° ë²¡í„° DB ê´€ë¦¬ í´ë˜ìŠ¤ (ì„ë² ë”© & ë²¡í„° DBí™”)
# =============================================================================

class MultiEmbeddingManager:
    def __init__(self, api_token):
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ë“¤
        self.embedding_models = {
            "embedding-gemma": {
                "name": "google/embeddinggemma-300m",
                "description": "ë‹¤êµ­ì–´ ì§€ì› SentenceTransformer ëª¨ë¸",
                "language": "Multilingual",
                "size": "~1.2G"
            },
            "ko-sroberta-multitask": {
                "name": "jhgan/ko-sroberta-multitask",
                "description": "í•œêµ­ì–´ íŠ¹í™” SentenceTransformer ëª¨ë¸",
                "language": "Korean",
                "size": "~300MB"
            },
            "paraphrase-multilingual": {
                "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                "description": "ë‹¤êµ­ì–´ ì§€ì› ê²½ëŸ‰ ëª¨ë¸",
                "language": "Multilingual",
                "size": "~420MB"
            },
            "ko-sentence-transformer": {
                "name": "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens",
                "description": "100ê°œ ì–¸ì–´ ì§€ì› BERT ê¸°ë°˜ ëª¨ë¸",
                "language": "Multilingual",
                "size": "~1.1GB"
            },
            "distiluse-multilingual": {
                "name": "sentence-transformers/distiluse-base-multilingual-cased-v2",
                "description": "ë‹¤êµ­ì–´ DistilUSE ëª¨ë¸ (ë¹ ë¦„)",
                "language": "Multilingual",
                "size": "~540MB"
            },
            "ko-electra": {
                "name": "bongsoo/kpf-sbert-128d",
                "description": "í•œêµ­ì–´ ELECTRA ê¸°ë°˜ ê²½ëŸ‰ ëª¨ë¸",
                "language": "Korean",
                "size": "~50MB"
            }
        }

        self.loaded_embeddings = {}
        self.current_model = None
        self.hf_api_token = api_token
        print(f"embedding manager with key: {self.hf_api_token}")

    def load_embedding_model(self, model_key: str) -> HuggingFaceEmbeddings:
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        if model_key in self.loaded_embeddings:
            return self.loaded_embeddings[model_key]

        if model_key not in self.embedding_models:
            raise ValueError(f"Unknown embedding model: {model_key}")

        model_info = self.embedding_models[model_key]
        print(f"Loading embedding model: {model_info['name']} ({model_info['size']})")
        print(f"my hf api token = {self.hf_api_token}")   
        try:

            if model_key == "embedding-gemma":
                
                # # Download from the ğŸ¤— Hub
                # model = SentenceTransformer("google/embeddinggemma-300m")
                embeddings = HuggingFaceEmbeddings(
                    model_name=model_info['name'],
                    model_kwargs={'device': 'cpu', "token": self.hf_api_token}
                )
            else:
                embeddings = HuggingFaceEmbeddings(
                    model_name=model_info['name'],
                    model_kwargs={'device': 'cpu', "token": self.hf_api_token}
                )
            self.loaded_embeddings[model_key] = embeddings
            self.current_model = model_key
            return embeddings
        except Exception as e:
            print(f"Failed to load {model_key}: {e}")
            # fallback to default model
            if model_key != "paraphrase-multilingual":
                return self.load_embedding_model("paraphrase-multilingual")
            raise e

    def get_available_models(self) -> Dict[str, str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return {key: f"{info['description']} ({info['language']}, {info['size']})"
                for key, info in self.embedding_models.items()}

    def get_current_model_info(self) -> str:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if self.current_model:
            info = self.embedding_models[self.current_model]
            return f"ğŸ¤– {info['description']} ({info['language']}, {info['size']})"
        return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ"

class VectorStoreManager:
    def __init__(self, embedding_model_key: str = "embedding-gemma", save_directory: str = "faiss_indexes", hf_api_token: str = None):
        self.embedding_manager = MultiEmbeddingManager(api_token=hf_api_token)
        
        self.vector_stores = {}  # ëª¨ë¸ë³„ ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
        self.current_vector_store = None
        self.current_model_key = embedding_model_key
        self.save_directory = save_directory
        self.embeddings = None
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.save_directory, exist_ok=True)

    def lazy_init_embedding(self, embedding_model_key):
        self.embeddings = self.embedding_manager.load_embedding_model(embedding_model_key)

    def switch_embedding_model(self, model_key: str) -> str:
        """ì„ë² ë”© ëª¨ë¸ ë³€ê²½"""
        try:
            print(f"Switching to embedding model: {model_key}")
            self.embeddings = self.embedding_manager.load_embedding_model(model_key)
            self.current_model_key = model_key

            # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ê°€ ìˆë‹¤ë©´ ì‚¬ìš©, ì—†ë‹¤ë©´ ìƒˆë¡œ ìƒì„± í•„ìš”
            if model_key in self.vector_stores:
                self.current_vector_store = self.vector_stores[model_key]
                return f"âœ… ëª¨ë¸ ë³€ê²½ ì™„ë£Œ: {self.embedding_manager.get_current_model_info()}"
            else:
                return f"âœ… ëª¨ë¸ ë³€ê²½ ì™„ë£Œ (ë²¡í„° ìŠ¤í† ì–´ ì¬ìƒì„± í•„ìš”): {self.embedding_manager.get_current_model_info()}"
        except Exception as e:
            return f"âŒ ëª¨ë¸ ë³€ê²½ ì‹¤íŒ¨: {str(e)}"

    def create_vector_store(self, documents: List[Document], model_key: str = None) -> FAISS:
        """ë²¡í„° ìŠ¤í† ì–´ ìƒì„± - Document ë©”íƒ€ë°ì´í„° ê°•í™”"""
        if model_key and model_key != self.current_model_key:
            self.switch_embedding_model(model_key)

        if self.embeddings is None:
            self.lazy_init_embedding(self.current_model_key)
        print(f"ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘... (ëª¨ë¸: {self.current_model_key})")
        
        # Document ë©”íƒ€ë°ì´í„° ê°•í™”
        enhanced_documents = self._enhance_document_metadata(documents)
        
        print(f"ë¬¸ì„œ {len(enhanced_documents)}ê°œì˜ ë©”íƒ€ë°ì´í„° ê°•í™” ì™„ë£Œ")
        
        try:
            vector_store = FAISS.from_documents(enhanced_documents, self.embeddings)
            self.vector_stores[self.current_model_key] = vector_store
            self.current_vector_store = vector_store
            
            # ë²¡í„° ìŠ¤í† ì–´ì— ì „ì²´ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í†µê³„ ì¶”ê°€
            self._add_vector_store_metadata(vector_store, enhanced_documents)
            
            return vector_store
        except Exception as e:
            print(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
            # fallback modelë¡œ ì¬ì‹œë„
            if self.current_model_key != "paraphrase-multilingual":
                print("ê¸°ë³¸ ëª¨ë¸ë¡œ ì¬ì‹œë„...")
                self.switch_embedding_model("paraphrase-multilingual")
                vector_store = FAISS.from_documents(enhanced_documents, self.embeddings)
                self.vector_stores[self.current_model_key] = vector_store
                self.current_vector_store = vector_store
                self._add_vector_store_metadata(vector_store, enhanced_documents)
                return vector_store
            raise e

    def add_documents(self, documents: List[Document]):
        """ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ì— ë¬¸ì„œ ì¶”ê°€ - Document ë©”íƒ€ë°ì´í„° ê°•í™”"""
        # Document ë©”íƒ€ë°ì´í„° ê°•í™”
        enhanced_documents = self._enhance_document_metadata(documents)
        
        print(f"ì¶”ê°€í•  ë¬¸ì„œ {len(enhanced_documents)}ê°œì˜ ë©”íƒ€ë°ì´í„° ê°•í™” ì™„ë£Œ")
        
        if self.current_vector_store:
            self.current_vector_store.add_documents(enhanced_documents)
            self.vector_stores[self.current_model_key] = self.current_vector_store
            
            # ë²¡í„° ìŠ¤í† ì–´ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            self._update_vector_store_metadata(self.current_vector_store, enhanced_documents)
        else:
            self.create_vector_store(enhanced_documents)

    def get_available_models(self) -> Dict[str, str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ ëª©ë¡"""
        return self.embedding_manager.get_available_models()

    def get_current_model_info(self) -> str:
        """í˜„ì¬ ëª¨ë¸ ì •ë³´"""
        return self.embedding_manager.get_current_model_info()
    
    def save_vector_store(self, index_name: str, model_key: str = None) -> tuple[bool, str]:
        """ë²¡í„° ìŠ¤í† ì–´ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if model_key is None:
            model_key = self.current_model_key
            
        if model_key not in self.vector_stores:
            return False, f"âŒ ì €ì¥í•  ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤: {model_key}"
        
        try:
            # FAISS ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ
            index_path = os.path.join(self.save_directory, f"{index_name}_{model_key}")
            
            index_dir = Path(index_path)
            index_dir.mkdir(exist_ok=True)

            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            vector_store = self.vector_stores[model_key]
            vector_store.save_local(index_path)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥ (ëª¨ë¸ ì •ë³´, ìƒì„±ì¼ì‹œ, ë¬¸ì„œ í†µê³„ ë“±)
            document_metadata = getattr(vector_store, '_document_metadata', {})
            metadata = {
                'model_key': model_key,
                'model_name': self.embedding_manager.embedding_models[model_key]['name'],
                'index_name': index_name,
                'created_at': __import__('datetime').datetime.now().isoformat(),
                'document_count': len(vector_store.docstore._dict) if hasattr(vector_store.docstore, '_dict') else 'unknown',
                'document_sources': document_metadata.get('sources', []),
                'document_types': document_metadata.get('types', []),
                'content_length_stats': document_metadata.get('content_stats', {}),
                'unique_metadata_keys': document_metadata.get('metadata_keys', []),
                'last_modified': __import__('datetime').datetime.now().isoformat()
            }
            
            metadata_path = os.path.join(self.save_directory, f"{index_name}_{model_key}_metadata.pkl")
            with open(metadata_path, 'wb') as f:
                pickle.dump(metadata, f)
            
            # Debugìš© JSON íŒŒì¼ë¡œë„ ì €ì¥
            json_metadata_path = os.path.join(self.save_directory, f"{index_name}_{model_key}_metadata_debug.json")
            with open(json_metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2, default=str)
            
            return True, f"âœ… ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ: {index_path}"
            
        except Exception as e:
            return False, f"âŒ ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì‹¤íŒ¨: {str(e)}"
    
    def load_vector_store(self, index_name: str, model_key: str = None) -> tuple[bool, str]:
        """ì €ì¥ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œ"""
        if model_key is None:
            model_key = self.current_model_key
        
        try:
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ ê²½ë¡œ
            index_path = os.path.join(self.save_directory, f"{index_name}_{model_key}")

            faiss_file = os.path.join(index_path, "index.faiss")
            pkl_file = os.path.join(index_path, "index.pkl")            
            if not os.path.exists(faiss_file):
                return False, f"âŒ ë²¡í„° ìŠ¤í† ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {index_path}"
            
            # í•´ë‹¹ ëª¨ë¸ì˜ ì„ë² ë”©ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¡œë“œ
            if model_key != self.current_model_key:
                self.switch_embedding_model(model_key)
            
            if self.embeddings is None:
                self.lazy_init_embedding(self.current_model_key)

            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            vector_store = FAISS.load_local(
                index_path, 
                self.embeddings, 
                allow_dangerous_deserialization=True
            )
            
            self.vector_stores[model_key] = vector_store
            self.current_vector_store = vector_store
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ (ìˆë‹¤ë©´)
            metadata_path = os.path.join(self.save_directory, f"{index_name}_{model_key}_metadata.pkl")
            metadata_info = ""
            if os.path.exists(metadata_path):
                with open(metadata_path, 'rb') as f:
                    metadata = pickle.load(f)
                    metadata_info = f" (ìƒì„±ì¼: {metadata.get('created_at', 'Unknown')}, ë¬¸ì„œìˆ˜: {metadata.get('document_count', 'Unknown')})"
            
            return True, f"âœ… ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ: {index_name}_{model_key}{metadata_info}"
            
        except Exception as e:
            return False, f"âŒ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
    
    def list_saved_indexes(self) -> Dict[str, Dict[str, Any]]:
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ëª©ë¡ ë°˜í™˜"""
        saved_indexes = {}
        
        if not os.path.exists(self.save_directory):
            return saved_indexes
        
        for filename in os.listdir(self.save_directory):
            if filename.endswith('.faiss'):
                # íŒŒì¼ëª…ì—ì„œ ì¸ë±ìŠ¤ëª…ê³¼ ëª¨ë¸í‚¤ ì¶”ì¶œ
                base_name = filename.replace('.faiss', '')
                # ë§ˆì§€ë§‰ '_'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
                parts = base_name.rsplit('_', 1)
                if len(parts) == 2:
                    index_name, model_key = parts
                    
                    # ë©”íƒ€ë°ì´í„° ì •ë³´ ë¡œë“œ
                    metadata_path = os.path.join(self.save_directory, f"{base_name}_metadata.pkl")
                    metadata = {}
                    if os.path.exists(metadata_path):
                        try:
                            with open(metadata_path, 'rb') as f:
                                metadata = pickle.load(f)
                        except:
                            pass
                    
                    if index_name not in saved_indexes:
                        saved_indexes[index_name] = {}
                    
                    saved_indexes[index_name][model_key] = {
                        'file_path': os.path.join(self.save_directory, filename),
                        'created_at': metadata.get('created_at', 'Unknown'),
                        'document_count': metadata.get('document_count', 'Unknown'),
                        'model_name': metadata.get('model_name', 'Unknown')
                    }
        
        return saved_indexes
    
    def delete_saved_index(self, index_name: str, model_key: str) -> str:
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ì‚­ì œ"""
        try:
            index_path = os.path.join(self.save_directory, f"{index_name}_{model_key}")
            metadata_path = os.path.join(self.save_directory, f"{index_name}_{model_key}_metadata.pkl")
            
            # FAISS ê´€ë ¨ íŒŒì¼ë“¤ ì‚­ì œ
            files_to_delete = [
                f"{index_path}.faiss",
                f"{index_path}.pkl",
                metadata_path
            ]
            
            deleted_files = []
            for file_path in files_to_delete:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    deleted_files.append(os.path.basename(file_path))
            
            if deleted_files:
                return f"âœ… ì¸ë±ìŠ¤ ì‚­ì œ ì™„ë£Œ: {', '.join(deleted_files)}"
            else:
                return f"âŒ ì‚­ì œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {index_name}_{model_key}"
                
        except Exception as e:
            return f"âŒ ì¸ë±ìŠ¤ ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
    
    def index_exists(self, index_name: str, model_key: str = None) -> bool:
        """ì €ì¥ëœ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        if model_key is None:
            model_key = self.current_model_key
        
        try:
            index_path = os.path.join(self.save_directory, f"{index_name}_{model_key}")
            faiss_file = os.path.join(index_path, "index.faiss")
            pkl_file = os.path.join(index_path, "index.pkl")
            
            # ë‘ íŒŒì¼ ëª¨ë‘ ì¡´ì¬í•´ì•¼ ìœ íš¨í•œ ì¸ë±ìŠ¤
            return os.path.exists(faiss_file) and os.path.exists(pkl_file)
        except Exception:
            return False
    
    def auto_save_after_creation(self, documents: List[Document], index_name: str, model_key: str = None) -> FAISS:
        """ë¬¸ì„œë¡œë¶€í„° ë²¡í„° ìŠ¤í† ì–´ ìƒì„± í›„ ìë™ ì €ì¥"""
        vector_store = self.create_vector_store(documents, model_key)
        save_result = self.save_vector_store(index_name, model_key)
        print(save_result)
        return vector_store


    def _enhance_document_metadata(self, documents: List[Document]) -> List[Document]:
        """Document ë©”íƒ€ë°ì´í„° ê°•í™”"""
        enhanced_docs = []
        
        for i, doc in enumerate(documents):
            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë³µì‚¬
            enhanced_metadata = doc.metadata.copy()
            
            # ê¸°ë³¸ ì •ë³´ ì¶”ê°€
            enhanced_metadata.update({
                'doc_index': i,
                'content_length': len(doc.page_content),
                'content_word_count': len(doc.page_content.split()),
                'processing_timestamp': __import__('datetime').datetime.now().isoformat(),
                'embedding_model': self.current_model_key,
                'embedding_model_name': self.embedding_manager.embedding_models[self.current_model_key]['name']
            })
            
            # ë‚´ìš© ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            content = doc.page_content
            enhanced_metadata.update({
                'has_korean': any(ord(char) >= 0xAC00 and ord(char) <= 0xD7A3 for char in content),
                'has_english': any(char.isascii() and char.isalpha() for char in content),
                'has_numbers': any(char.isdigit() for char in content),
                'line_count': len(content.split('\n')),
                'paragraph_count': len([p for p in content.split('\n\n') if p.strip()])
            })
            
            # ì†ŒìŠ¤ ì •ë³´ ì •ê·œí™”
            if 'source' not in enhanced_metadata and 'file_path' in enhanced_metadata:
                enhanced_metadata['source'] = enhanced_metadata['file_path']
            
            # ë¬¸ì„œ íƒ€ì… ì¶”ë¡ 
            doc_type = self._infer_document_type(enhanced_metadata, content)
            enhanced_metadata['inferred_type'] = doc_type
            
            enhanced_docs.append(Document(
                page_content=content,
                metadata=enhanced_metadata
            ))
        
        return enhanced_docs
    
    def _infer_document_type(self, metadata: dict, content: str) -> str:
        """ë¬¸ì„œ íƒ€ì… ì¶”ë¡ """
        # íŒŒì¼ í™•ì¥ì ê¸°ë°˜ ì¶”ë¡ 
        source = metadata.get('source', '')
        if source.endswith('.pdf'):
            return 'PDF'
        elif source.endswith(('.txt', '.md')):
            return 'Text'
        elif source.endswith('.html'):
            return 'HTML'
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¶”ë¡ 
        if 'page' in metadata:
            return 'PDF'
        elif 'url' in metadata or source.startswith('http'):
            return 'Web'
        
        # ë‚´ìš© ê¸°ë°˜ ì¶”ë¡ 
        if '<!DOCTYPE html>' in content or '<html>' in content:
            return 'HTML'
        elif len(content.split('\n')) > 50 and len(content.split()) > 1000:
            return 'Long Document'
        else:
            return 'Text'
    
    def _add_vector_store_metadata(self, vector_store: FAISS, documents: List[Document]):
        """ë²¡í„° ìŠ¤í† ì–´ì— ì „ì²´ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í†µê³„ ì¶”ê°€"""
        # ë¬¸ì„œ í†µê³„ ê³„ì‚°
        sources = list(set(doc.metadata.get('source', 'unknown') for doc in documents))
        doc_types = list(set(doc.metadata.get('inferred_type', 'unknown') for doc in documents))
        
        content_lengths = [len(doc.page_content) for doc in documents]
        content_stats = {
            'min_length': min(content_lengths) if content_lengths else 0,
            'max_length': max(content_lengths) if content_lengths else 0,
            'avg_length': sum(content_lengths) / len(content_lengths) if content_lengths else 0,
            'total_length': sum(content_lengths)
        }
        
        # ëª¨ë“  ë©”íƒ€ë°ì´í„° í‚¤ ìˆ˜ì§‘
        all_metadata_keys = set()
        for doc in documents:
            all_metadata_keys.update(doc.metadata.keys())
        
        # ë²¡í„° ìŠ¤í† ì–´ì— ë©”íƒ€ë°ì´í„° ì €ì¥
        vector_store._document_metadata = {
            'sources': sources,
            'types': doc_types,
            'content_stats': content_stats,
            'metadata_keys': list(all_metadata_keys),
            'document_count': len(documents),
            'created_at': __import__('datetime').datetime.now().isoformat()
        }
    
    def _update_vector_store_metadata(self, vector_store: FAISS, new_documents: List[Document]):
        """ë²¡í„° ìŠ¤í† ì–´ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ë¬¸ì„œ ì¶”ê°€ ì‹œ)"""
        existing_metadata = getattr(vector_store, '_document_metadata', {})
        
        # ìƒˆë¡œìš´ ë¬¸ì„œë“¤ì˜ í†µê³„ ê³„ì‚°
        new_sources = list(set(doc.metadata.get('source', 'unknown') for doc in new_documents))
        new_types = list(set(doc.metadata.get('inferred_type', 'unknown') for doc in new_documents))
        
        new_content_lengths = [len(doc.page_content) for doc in new_documents]
        
        # ê¸°ì¡´ í†µê³„ì™€ í•©ë³‘
        all_sources = list(set(existing_metadata.get('sources', []) + new_sources))
        all_types = list(set(existing_metadata.get('types', []) + new_types))
        
        # ë©”íƒ€ë°ì´í„° í‚¤ ì—…ë°ì´íŠ¸
        all_metadata_keys = set(existing_metadata.get('metadata_keys', []))
        for doc in new_documents:
            all_metadata_keys.update(doc.metadata.keys())
        
        # ì „ì²´ ë¬¸ì„œ ìˆ˜ ì—…ë°ì´íŠ¸
        total_docs = existing_metadata.get('document_count', 0) + len(new_documents)
        
        # ì—…ë°ì´íŠ¸ëœ ë©”íƒ€ë°ì´í„° ì €ì¥
        vector_store._document_metadata = {
            'sources': all_sources,
            'types': all_types,
            'content_stats': {
                'new_docs_count': len(new_documents),
                'new_docs_total_length': sum(new_content_lengths),
                'new_docs_avg_length': sum(new_content_lengths) / len(new_content_lengths) if new_content_lengths else 0
            },
            'metadata_keys': list(all_metadata_keys),
            'document_count': total_docs,
            'last_updated': __import__('datetime').datetime.now().isoformat(),
            'created_at': existing_metadata.get('created_at', __import__('datetime').datetime.now().isoformat())
        }
    
    def get_document_metadata_summary(self) -> Dict[str, Any]:
        """í˜„ì¬ ë²¡í„° ìŠ¤í† ì–´ì˜ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìš”ì•½ ë°˜í™˜"""
        if not self.current_vector_store:
            return {'error': 'ë¡œë“œëœ ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        metadata = getattr(self.current_vector_store, '_document_metadata', {})
        if not metadata:
            return {'error': 'ë©”íƒ€ë°ì´í„° ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        return {
            'ë¬¸ì„œ ìˆ˜': metadata.get('document_count', 0),
            'ì†ŒìŠ¤ ëª©ë¡': metadata.get('sources', []),
            'ë¬¸ì„œ íƒ€ì…': metadata.get('types', []),
            'ë‚´ìš© í†µê³„': metadata.get('content_stats', {}),
            'ë©”íƒ€ë°ì´í„° í‚¤': metadata.get('metadata_keys', []),
            'ìƒì„±ì¼': metadata.get('created_at', 'Unknown'),
            'ìµœê·¼ ì—…ë°ì´íŠ¸': metadata.get('last_updated', metadata.get('created_at', 'Unknown'))
        }
    
    def search_documents_by_metadata(self, metadata_key: str, metadata_value: Any, top_k: int = 10) -> List[Document]:
        """ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.current_vector_store:
            return []
        
        # FAISSì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        all_docs = []
        if hasattr(self.current_vector_store, 'docstore') and hasattr(self.current_vector_store.docstore, '_dict'):
            for doc_id, doc in self.current_vector_store.docstore._dict.items():
                if metadata_key in doc.metadata and doc.metadata[metadata_key] == metadata_value:
                    all_docs.append(doc)
        
        return all_docs[:top_k]


if __name__ == "__main__":
    vector_store_manager = VectorStoreManager()