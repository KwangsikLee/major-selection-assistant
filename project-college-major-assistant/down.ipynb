{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import html\n",
    "import pathlib\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "BASE_URL = \"https://oku.korea.ac.kr/oku/cms/FR_CON/index.do?MENU_ID=820\"\n",
    "DOWNLOAD_DIR = pathlib.Path(\"korea_univ_guides\")\n",
    "TIMEOUT = 30\n",
    "MAX_WORKERS = 6  # 병렬 다운로드 수 (requests만 사용 -> 간단히 순차로도 충분)\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) PythonRequestsDownloader/1.0\",\n",
    "    \"Referer\": BASE_URL,\n",
    "    \"Accept\": \"*/*\",\n",
    "}\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        connect=5,\n",
    "        read=5,\n",
    "        backoff_factor=0.6,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\", \"HEAD\", \"OPTIONS\"]\n",
    "    )\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "    s.headers.update(HEADERS)\n",
    "    return s\n",
    "\n",
    "def safe_filename(name: str, fallback: str = \"file\"):\n",
    "    name = html.unescape(name).strip()\n",
    "    # 줄바꿈/탭 제거\n",
    "    name = re.sub(r\"[\\r\\n\\t]+\", \" \", name)\n",
    "    # 너무 긴 이름 커팅\n",
    "    name = name[:180]\n",
    "    # 파일 시스템에 위험한 문자 제거 (한글/영문/숫자/공백/._-()만 허용)\n",
    "    name = re.sub(r\"[^0-9A-Za-z가-힣\\.\\-_\\(\\) ]+\", \"_\", name)\n",
    "    name = name.strip(\" .\")\n",
    "    return name or fallback\n",
    "\n",
    "def parse_content_disposition(cd_header: str):\n",
    "    \"\"\"\n",
    "    Content-Disposition 예: attachment; filename=\"문과대학_국어국문학과_가이드.pdf\"\n",
    "    혹은 filename*=UTF-8''... 형태\n",
    "    \"\"\"\n",
    "    if not cd_header:\n",
    "        return None\n",
    "    # RFC 5987 filename* 우선\n",
    "    m = re.search(r\"filename\\*\\s*=\\s*UTF-8''([^;]+)\", cd_header, flags=re.I)\n",
    "    if m:\n",
    "        try:\n",
    "            from urllib.parse import unquote\n",
    "            return unquote(m.group(1))\n",
    "        except Exception:\n",
    "            pass\n",
    "    m = re.search(r'filename\\s*=\\s*\"([^\"]+)\"', cd_header, flags=re.I)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    m = re.search(r\"filename\\s*=\\s*([^;]+)\", cd_header, flags=re.I)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def extract_department_label(a_tag):\n",
    "    \"\"\"\n",
    "    a_tag(“PDF 다운”) 주변에서 학과/전공명을 추정.\n",
    "    HTML 구조상 '  * 학과명' 텍스트 라인이 있고 그 뒤에 [동영상] [PDF 다운] [홈페이지]가 이어짐.\n",
    "    → a_tag의 부모 블록에서 직전 텍스트 노드를 찾아본다.\n",
    "    \"\"\"\n",
    "    # 형제/부모를 거슬러 올라가며 최근에 등장한 학과명 라인을 찾는다.\n",
    "    # 단순하지만 실무에 충분한 휴리스틱.\n",
    "    label = None\n",
    "    # 1) 바로 이전의 텍스트 노드\n",
    "    prev = a_tag.find_previous(string=True)\n",
    "    if prev:\n",
    "        t = prev.strip()\n",
    "        # '학과/학부/부' 등의 키워드가 있는 경우만 사용\n",
    "        if any(kw in t for kw in (\"학과\", \"학부\", \"부\", \"과\")) and len(t) <= 30:\n",
    "            label = t\n",
    "\n",
    "    # 2) 못 찾으면 더 넓게: 이전에 나온 굵은 제목 등의 텍스트\n",
    "    if not label:\n",
    "        parent_text = a_tag.find_parent().get_text(\" \", strip=True)\n",
    "        # 새 줄 또는 불릿 앞 부분을 추정\n",
    "        # 예: \"국어국문학과 동영상 PDF 다운 홈페이지\"\n",
    "        m = re.match(r\"^([^\\s]+?(학과|학부|부))\\b\", parent_text)\n",
    "        if m:\n",
    "            label = m.group(1)\n",
    "\n",
    "    return label\n",
    "\n",
    "def discover_pdf_links(session):\n",
    "    print(\"[*] 페이지 가져오는 중:\", BASE_URL)\n",
    "    r = session.get(BASE_URL, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    results = []\n",
    "    college_idx = 0\n",
    "\n",
    "    # 단과대학 단위 블록 찾기\n",
    "    for block in soup.select(\"div.college_info\"):\n",
    "        # 단과대학 이름\n",
    "        college_name_tag = block.find(\"strong\")\n",
    "        if not college_name_tag:\n",
    "            continue\n",
    "        college_idx += 1\n",
    "        college_name = college_name_tag.get_text(strip=True)\n",
    "\n",
    "        # 학과 링크들\n",
    "        dept_links = block.find_all(\"a\", string=\"PDF 다운\")\n",
    "        dept_idx = 0\n",
    "        for a in dept_links:\n",
    "            dept_idx += 1\n",
    "            href = a.get(\"href\")\n",
    "            if not href or \"FileDown.do\" not in href:\n",
    "                continue\n",
    "\n",
    "            # 학과명 추출\n",
    "            parent_text = a.find_parent().get_text(\" \", strip=True)\n",
    "            # \"화공생명공학과 PDF 다운\" → \"화공생명공학과\"\n",
    "            dept_name = parent_text.replace(\"PDF 다운\", \"\").strip()\n",
    "            dept_name = dept_name.split()[0]\n",
    "\n",
    "            results.append({\n",
    "                \"url\": urljoin(BASE_URL, href),\n",
    "                \"college_idx\": college_idx,\n",
    "                \"college_name\": college_name,\n",
    "                \"dept_idx\": dept_idx,\n",
    "                \"dept_name\": dept_name\n",
    "            })\n",
    "\n",
    "    # 중복 제거 (url 기준)\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for x in results:\n",
    "        if x[\"url\"] not in seen:\n",
    "            seen.add(x[\"url\"])\n",
    "            uniq.append(x)\n",
    "    print(f\"[*] 발견된 PDF 링크: {len(uniq)}개\")\n",
    "    return uniq\n",
    "\n",
    "    print(f\"[*] 발견된 PDF 링크: {len(results)}개\")\n",
    "    return results\n",
    "\n",
    "def discover_pdf_links2(session: requests.Session):\n",
    "    print(\"[*] 페이지 가져오는 중:\", BASE_URL)\n",
    "    r = session.get(BASE_URL, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        text = (a.get_text() or \"\").strip()\n",
    "        href = a.get(\"href\") or \"\"\n",
    "        if (\"PDF 다운\" in text) or (\"FileDown.do\" in href):\n",
    "            full_url = urljoin(BASE_URL, href)\n",
    "            # 필터링: 실제 다운로드 엔드포인트만\n",
    "            if \"FileDown.do\" in full_url:\n",
    "                label = extract_department_label(a) or \"\"\n",
    "                # 쿼리에서 MAJOR_SEQ, MAJOR_SUB_SEQ 추출\n",
    "                q = parse_qs(urlparse(full_url).query)\n",
    "                major_seq = q.get(\"MAJOR_SEQ\", [\"\"])[0]\n",
    "                major_sub = q.get(\"MAJOR_SUB_SEQ\", [\"\"])[0]\n",
    "                links.append({\n",
    "                    \"url\": full_url,\n",
    "                    \"label\": label,\n",
    "                    \"major_seq\": major_seq,\n",
    "                    \"major_sub\": major_sub\n",
    "                })\n",
    "\n",
    "    # 중복 제거 (url 기준)\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for x in links:\n",
    "        if x[\"url\"] not in seen:\n",
    "            seen.add(x[\"url\"])\n",
    "            uniq.append(x)\n",
    "    print(f\"[*] 발견된 PDF 링크: {len(uniq)}개\")\n",
    "    return uniq\n",
    "\n",
    "def ensure_pdf_suffix(name: str):\n",
    "    return name if name.lower().endswith(\".pdf\") else (name + \".pdf\")\n",
    "\n",
    "def download_one(session: requests.Session, item: dict, outdir: pathlib.Path):\n",
    "    url = item[\"url\"]\n",
    "    label = item[\"label\"]\n",
    "    seq = f\"{item['major_seq']}_{item['major_sub']}\".strip(\"_\")\n",
    "\n",
    "\n",
    "    # 우선 임시 파일명\n",
    "    base_name = safe_filename(label, fallback=f\"guide_{seq}\")\n",
    "    filename = ensure_pdf_suffix(base_name)\n",
    "    dest = outdir / filename\n",
    "\n",
    "    # 원하는 포맷: 06-공과대학-01.화공생명공학.pdf\n",
    "    filename = f\"{item['college_idx']:02d}-{safe_filename(item['college_name'])}-{item['dept_idx']:02d}.{safe_filename(item['dept_name'])}.pdf\"\n",
    "    dest = outdir / filename\n",
    "\n",
    "\n",
    "    # 이미 있으면 스킵\n",
    "    if dest.exists() and dest.stat().st_size > 0:\n",
    "        print(f\"[=] 존재함, 건너뜀: {dest.name}\")\n",
    "        return dest\n",
    "\n",
    "    print(f\"[>] 다운로드 시작: {dest.name}\")\n",
    "    with session.get(url, stream=True, timeout=TIMEOUT) as resp:\n",
    "        resp.raise_for_status()\n",
    "        # Content-Disposition에서 실제 파일명 가져오기\n",
    "        cd = resp.headers.get(\"Content-Disposition\", \"\")\n",
    "        cd_name = parse_content_disposition(cd)\n",
    "        if cd_name:\n",
    "            cd_name = safe_filename(cd_name)\n",
    "            if cd_name:\n",
    "                dest = outdir / ensure_pdf_suffix(cd_name)\n",
    "\n",
    "        tmp = dest.with_suffix(dest.suffix + \".part\")\n",
    "        tmp.parent.mkdir(parents=True, exist_ok=True)\n",
    "        total = 0\n",
    "        with open(tmp, \"wb\") as f:\n",
    "            for chunk in resp.iter_content(chunk_size=1024 * 64):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    total += len(chunk)\n",
    "        os.replace(tmp, dest)\n",
    "        print(f\"[✔] 완료: {dest.name} ({total/1024:.1f} KB)\")\n",
    "        return dest\n",
    "\n",
    "def main():\n",
    "    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    session = make_session()\n",
    "    links = discover_pdf_links(session)\n",
    "\n",
    "    if not links:\n",
    "        print(\"[!] PDF 링크를 찾지 못했습니다. 페이지 구조 변경 여부를 확인하세요.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 순차 다운로드 (안정성 위주). 필요시 threading으로 바꿔도 됨.\n",
    "    ok, fail = 0, 0\n",
    "    for item in links:\n",
    "        try:\n",
    "            download_one(session, item, DOWNLOAD_DIR)\n",
    "            ok += 1\n",
    "            # 예의상 서버 부하 방지\n",
    "            time.sleep(0.2)\n",
    "        except Exception as e:\n",
    "            fail += 1\n",
    "            print(f\"[x] 실패: {item['url']} -> {e}\")\n",
    "\n",
    "    print(f\"\\n[*] 완료 요약: 성공 {ok}건, 실패 {fail}건\")\n",
    "    print(f\"[*] 저장 위치: {DOWNLOAD_DIR.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af3987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34309b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "\n",
    "def rename_pdf_files(folder: str):\n",
    "    folder_path = Path(folder)\n",
    "    for file in folder_path.glob(\"*.pdf\"):\n",
    "        name = file.stem  # 확장자 제외\n",
    "        # \"_EC_8B_AC\" 같은 패턴을 \"%EC%8B%AC\"으로 변환\n",
    "        encoded = re.sub(r\"_([0-9A-Fa-f]{2})\", r\"%\\1\", name)\n",
    "        try:\n",
    "            decoded = urllib.parse.unquote(encoded)\n",
    "        except Exception:\n",
    "            print(f\"[x] 디코딩 실패: {file.name}\")\n",
    "            continue\n",
    "\n",
    "        # 새로운 파일명 (원래 있던 접두 숫자 유지)\n",
    "        new_name = decoded + file.suffix\n",
    "        new_path = file.with_name(new_name)\n",
    "\n",
    "        # 같은 이름 파일이 이미 있으면 건너뜀\n",
    "        if new_path.exists():\n",
    "            print(f\"[=] 이미 존재: {new_path.name}, 건너뜀\")\n",
    "            continue\n",
    "\n",
    "        file.rename(new_path)\n",
    "        print(f\"[✔] {file.name} -> {new_path.name}\")\n",
    "\n",
    "rename_pdf_files(\"./korea_univ_guides\")        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
