#!/usr/bin/env python3
"""
CollegeQASystem - êµ¬ì¶•ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•œ ì§ˆë¬¸ë‹µë³€ ì „ë‹´ í´ëž˜ìŠ¤

Author: kwangsiklee  
Version: 0.1.0
"""

import json
import sys
from pathlib import Path
from typing import Dict, Any

# LangChain imports
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# a_my_rag_module ëª¨ë“ˆì„ importí•˜ê¸° ìœ„í•œ ê²½ë¡œ ì„¤ì •
# í˜„ìž¬ íŒŒì¼: /project-college-major-assistant/src/college_qa_system.py
# ëª©í‘œ ê²½ë¡œ: /AI-Study/a_my_rag_module
sys.path.append(str(Path(__file__).parent.parent.parent))
from a_my_rag_module import VectorStoreManager
from a_my_rag_module.retriever import HybridRetrieverWrapper


class CollegeQASystem:
    """êµ¬ì¶•ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©í•œ ì§ˆë¬¸ë‹µë³€ ì „ë‹´ í´ëž˜ìŠ¤"""
    
    # ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • ìƒìˆ˜
    DEFAULT_INDEX_NAME = "college_guide"
    DEFAULT_MODEL_KEY = "embedding-gemma"
    
    def __init__(self, vector_db_dir: str):
        self.vector_db_dir = Path(vector_db_dir)
        
        # LLM ì„¤ì • (í•„ìš”ì‹œ ì§€ì—° ë¡œë”©)
        self.llm = None
        
        # ë²¡í„° ìŠ¤í† ì–´ì™€ QA ì²´ì¸
        self.vector_store = None
        self.qa_chain = None
        
        # VectorStoreManager
        self.vector_manager = None
        
        # HybridRetrieverWrapper
        self.hybrid_retriever_wrapper = None
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.setup_prompt_template()
        
        print(f"CollegeQASystem ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ë²¡í„° DB ë””ë ‰í† ë¦¬: {self.vector_db_dir}")
    
    def initialize_llm_components(self):
        """LLM êµ¬ì„±ìš”ì†Œ ì§€ì—° ì´ˆê¸°í™”"""
        if self.llm is None:
            print("ðŸ¤– LLM êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™” ì¤‘...")
            
            # OpenAI ì„¤ì •
            self.llm = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0.2
            )
    
    def setup_prompt_template(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •"""
        self.prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""ë‹¹ì‹ ì€ ê³ ë“±í•™ìƒë“¤ì˜ ëŒ€í•™êµ ì „ê³µ ì„ íƒì„ ë„ì™€ì£¼ëŠ” ì „ë¬¸ ìƒë‹´ì‚¬ìž…ë‹ˆë‹¤.

ì•„ëž˜ ëŒ€í•™êµ í•™ê³¼ ì•ˆë‚´ ìžë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ì°¸ê³  ìžë£Œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
1. ê³ ë“±í•™ìƒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”
2. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”  
3. ìž¥í™©í•˜ì§€ ì•Šê²Œ í‘œí˜„í•´ì£¼ì„¸ìš”
4. ì¹œê·¼í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”
5. ë‹µë³€ì€ ì˜¤ì§ ì°¸ê³ ìžë£Œì— ìžˆëŠ” ë‚´ìš©ì— í•œí•´ì„œë§Œ í•´ì•¼ í•œë‹¤.
5. ì°¸ê³  ìžë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì •ë³´ê°€ ì—†ì–´ ë‹µë³€ëª»í•´ ë¯¸ì•ˆí•˜ë‹¤ê³  í•˜ê³  ëë‚¸ë‹¤
6. ì°¸ê³  ìžë£Œì— ìžˆëŠ” ë‹µë³€ì€ ê·¼ê±°ë¥¼ í‘œì‹œí•´ì£¼ì„¸ìš”.
ë‹µë³€:"""
        )
    
    def initialize_vector_manager(self):
        """VectorStoreManager ì§€ì—° ì´ˆê¸°í™”"""
        if self.vector_manager is None:
            import os
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ HF API í† í° ê°€ì ¸ì˜¤ê¸° (ìžˆë‹¤ë©´)
            hf_token = os.getenv('HF_API_TOKEN') or os.getenv('HUGGINGFACEHUB_API_TOKEN')
            
            # VectorStoreManager ì´ˆê¸°í™”
            self.vector_manager = VectorStoreManager(
                embedding_model_key=self.DEFAULT_MODEL_KEY, 
                save_directory=str(self.vector_db_dir),
                hf_api_token=hf_token if hf_token is not None else ""
            )
    
    def load_vector_store(self):
        """ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ"""
        try:
            print("ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
            
            # LLM êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™”
            self.initialize_llm_components()
            
            # VectorStoreManager ì´ˆê¸°í™”
            self.initialize_vector_manager()
            if self.vector_manager is not None:
                # VectorStoreManagerë¡œ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„
                result, message = self.vector_manager.load_vector_store(
                    self.DEFAULT_INDEX_NAME, 
                    self.DEFAULT_MODEL_KEY
                )
                
                if result:
                    self.vector_store = self.vector_manager.current_vector_store                    
                    print(f"   {message}")                    
                    # HybridRetrieverWrapper ì´ˆê¸°í™”
                    self.setup_hybrid_retriever()
                
            if not self.vector_store:
                # VectorStoreManager ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‹œë„
                print(f"âš ï¸ VectorStoreManager ë¡œë“œ ì‹¤íŒ¨: {message}")
                raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # QA ì²´ì¸ ì„¤ì •
            self.setup_qa_chain()
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = self.vector_db_dir / "metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                print(f"   ìƒì„±ì¼: {metadata.get('created_at', 'Unknown')}")
                print(f"   ë¬¸ì„œ ìˆ˜: {metadata.get('total_documents', 'Unknown')}")
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def setup_hybrid_retriever(self):
        """HybridRetrieverWrapper ì„¤ì •"""
        try:
            if self.vector_store is not None:
                self.hybrid_retriever_wrapper = HybridRetrieverWrapper(
                    vector_store=self.vector_store,
                    search_method="ensemble"
                )
            else:
                raise ValueError("vector_storeê°€ Noneìž…ë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ HybridRetrieverWrapper ì„¤ì • ì‹¤íŒ¨: {e}")
            print("   ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.hybrid_retriever_wrapper = None
    
    def setup_qa_chain(self):
        """QA ì²´ì¸ ì„¤ì •"""
        if self.vector_store is None:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ë¦¬íŠ¸ë¦¬ë²„ ì„ íƒ
        retriever = self.hybrid_retriever_wrapper or self.vector_store.as_retriever(
            search_type="similarity", search_kwargs={"k": 3}
        )
        
        # QA ì²´ì¸ ìƒì„±
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": self.prompt_template},
            return_source_documents=True
        )
        
        retriever_name = "HybridRetrieverWrapper" if self.hybrid_retriever_wrapper else "ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰ê¸°"
        print(f"âœ… QA ì²´ì¸ ì„¤ì • ì™„ë£Œ ({retriever_name})")
    
    def query(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        if self.qa_chain is None:
            raise ValueError("QA ì²´ì¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•˜ì„¸ìš”.")
        
        try:
            print(f"ðŸ” ì§ˆë¬¸ ì²˜ë¦¬: {question}")
            
            # QA ì²´ì¸ìœ¼ë¡œ ì§ˆë¬¸ ì²˜ë¦¬
            result = self.qa_chain.invoke({"query": question})
            
            # ë‹µë³€ ì¶”ì¶œ
            answer = result.get("result", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            source_docs = result.get("source_documents", [])
            
            # ê²€ìƒ‰ëœ context ë¡œê·¸ ì¶œë ¥
            retriever_type = "HybridRetrieverWrapper (Hybrid + Reranking)" if self.hybrid_retriever_wrapper else "ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰ê¸°"
            print(f"\nðŸ“– {retriever_type}ê°€ ê²€ìƒ‰í•œ Context ì •ë³´:")
            print("=" * 80)
            for i, doc in enumerate(source_docs):
                metadata = doc.metadata
                content = doc.page_content
                print(f"[Context {i+1}]")
                print(f"ì¶œì²˜: {metadata.get('source', 'Unknown')} (íŽ˜ì´ì§€ {metadata.get('page', '?')})")
                print(f"ë‚´ìš© ê¸¸ì´: {len(content)} ê¸€ìž")
                print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content[:200]}..." if len(content) > 200 else f"ì „ì²´ ë‚´ìš©: {content}")
                print("-" * 40)
            print("=" * 80)
            
            # ì†ŒìŠ¤ ì •ë³´ ìƒì„±
            sources = []
            for doc in source_docs:
                metadata = doc.metadata
                source_info = f"{metadata.get('source', 'Unknown')} (íŽ˜ì´ì§€ {metadata.get('page', '?')})"
                if source_info not in sources:
                    sources.append(source_info)
            
            print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ (ì°¸ê³  ìžë£Œ: {len(sources)}ê°œ)")
            
            return {
                "question": question,
                "answer": answer,
                "sources": sources,
                "source_documents": source_docs
            }
            
        except Exception as e:
            error_msg = f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            print(f"âŒ {error_msg}")
            
            return {
                "question": question,
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. {error_msg}",
                "sources": [],
                "source_documents": []
            }
    
    def switch_reranker(self, reranker_model: str) -> str:
        """Reranker ëª¨ë¸ ë³€ê²½"""
        if not self.hybrid_retriever_wrapper:
            return "âš ï¸ HybridRetrieverWrapperê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        return self.hybrid_retriever_wrapper.switch_reranker(reranker_model)
    
    def get_retriever_info(self) -> str:
        """í˜„ìž¬ ê²€ìƒ‰ê¸° ì •ë³´ ë°˜í™˜"""
        if not self.hybrid_retriever_wrapper:
            return "ðŸ“Š í˜„ìž¬ ê²€ìƒ‰ê¸°: ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰ê¸°"
        
        return self.hybrid_retriever_wrapper.get_retriever_info()