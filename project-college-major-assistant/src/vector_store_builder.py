#!/usr/bin/env python3
"""
VectorStoreBuilder - PDF ì²˜ë¦¬ ë° ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì „ë‹´ í´ë˜ìŠ¤

Author: kwangsiklee  
Version: 0.1.0
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
import json
from datetime import datetime
import gc

# LangChain imports
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

# a_my_rag_module ëª¨ë“ˆì„ importí•˜ê¸° ìœ„í•œ ê²½ë¡œ ì„¤ì •
# í˜„ì¬ íŒŒì¼: /project-college-major-assistant/src/vector_store_builder.py
# ëª©í‘œ ê²½ë¡œ: /AI-Study/a_my_rag_module
sys.path.append(str(Path(__file__).parent.parent.parent))
from a_my_rag_module import PDFImageExtractor, KoreanOCR, VectorStoreManager


class VectorStoreBuilder:
    """PDF ì²˜ë¦¬ ë° ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì „ë‹´ í´ë˜ìŠ¤"""
    
    # ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • ìƒìˆ˜
    DEFAULT_INDEX_NAME = "college_guide"
    DEFAULT_MODEL_KEY = "embedding-gemma"
    
    def __init__(self, pdf_dir: str, temp_images_dir: str, vector_db_dir: str):
        self.pdf_dir = Path(pdf_dir)
        self.temp_images_dir = Path(temp_images_dir)
        self.vector_db_dir = Path(vector_db_dir)
        
        # temp_texts ë””ë ‰í† ë¦¬ ì¶”ê°€
        self.temp_texts_dir = Path(temp_images_dir).parent / "temp_texts"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.temp_images_dir.mkdir(exist_ok=True)
        self.temp_texts_dir.mkdir(exist_ok=True)
        self.vector_db_dir.mkdir(exist_ok=True)
        
        # PDF ì²˜ë¦¬ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self.pdf_extractor = PDFImageExtractor(dpi=100, max_size=2048)
        self.ocr = KoreanOCR()
        
        # VectorStoreManager 
        self.vector_manager = None

        # í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )
        
        # ë²¡í„° ìŠ¤í† ì–´ (êµ¬ì¶• ì‹œì—ë§Œ ì‚¬ìš©)
        self.vector_store = None
        
        print(f"VectorStoreBuilder ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"PDF ë””ë ‰í† ë¦¬: {self.pdf_dir}")
        print(f"ë²¡í„° DB ë””ë ‰í† ë¦¬: {self.vector_db_dir}")
        
    def force_memory_cleanup(self):
        """ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (PyTorch)
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            if torch.mps.is_available():
                torch.mps.empty_cache()
        except ImportError:
            pass
    
    def initialize_vector_manager(self):
        """Vector Manager ì§€ì—° ì´ˆê¸°í™”"""
        if self.vector_manager ==  None:        
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ HF API í† í° ê°€ì ¸ì˜¤ê¸° (ìˆë‹¤ë©´)
            hf_token = os.getenv('HF_API_TOKEN') or os.getenv('HUGGINGFACEHUB_API_TOKEN')
            
            # VectorStoreManager ì´ˆê¸°í™” (í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ì‚¬ìš©)
            self.vector_manager = VectorStoreManager(
                embedding_model_key=self.DEFAULT_MODEL_KEY, 
                save_directory=str(self.vector_db_dir),
                hf_api_token=hf_token if hf_token is not None else ""
            )
    
    def vector_store_exists(self) -> bool:
        """ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        self.initialize_vector_manager()
        if self.vector_manager:
            # VectorStoreManagerì˜ index_exists ë©”ì„œë“œ ì‚¬ìš©
            return self.vector_manager.index_exists(self.DEFAULT_INDEX_NAME, self.DEFAULT_MODEL_KEY)
        else:
            return False
    
    def build_vector_store(self, progress_callback: Optional[Callable] = None):
        """PDF íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• - 2ë‹¨ê³„ ì ‘ê·¼ë²•"""
        try:
            print("\nğŸ”„ ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì‹œì‘ (2ë‹¨ê³„ ì ‘ê·¼ë²•)")
            print("=" * 60)
            
            if progress_callback:
                progress_callback("PDF íŒŒì¼ ëª©ë¡ í™•ì¸ ì¤‘...")
            
            # PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            pdf_files = list(self.pdf_dir.glob("*.pdf"))
            
            if not pdf_files:
                raise ValueError(f"PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.pdf_dir}")
            
            print(f"ğŸ“„ ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
            
            # Phase 1: OCR ì²˜ë¦¬ ë° Document ì €ì¥
            if progress_callback:
                progress_callback("Phase 1: OCR ì²˜ë¦¬ ë° ë¬¸ì„œ ì €ì¥ ì¤‘...")
            
            processed_pdfs = self._phase1_ocr_processing(pdf_files, progress_callback)
            
            if not processed_pdfs:
                raise ValueError("OCR ì²˜ë¦¬ëœ PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # Phase 2: ë°°ì¹˜ ë²¡í„° ìŠ¤í† ì–´ ì¶”ê°€
            if progress_callback:
                progress_callback("Phase 2: ë²¡í„° ìŠ¤í† ì–´ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")
                
            success = self._phase2_batch_vector_addition(processed_pdfs, progress_callback)
            
            if not success:
                raise ValueError("ë²¡í„° ìŠ¤í† ì–´ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨")
            
            print("\nâœ… 2ë‹¨ê³„ ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            return False
    
    def _phase1_ocr_processing(self, pdf_files: List[Path], progress_callback: Optional[Callable] = None) -> List[str]:
        """
        Phase 1: PDF â†’ Image â†’ OCR â†’ Document ì €ì¥
        
        Args:
            pdf_files: ì²˜ë¦¬í•  PDF íŒŒì¼ ë¦¬ìŠ¤íŠ¸
            progress_callback: ì§„í–‰ìƒí™© ì½œë°± í•¨ìˆ˜
            
        Returns:
            List[str]: ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ PDF íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
        """
        print("\nğŸ“‹ Phase 1: OCR ì²˜ë¦¬ ë° ë¬¸ì„œ ì €ì¥")
        print("-" * 40)
        
        processed_pdfs = []
        
        # ìƒ˜í”Œë¡œ ì²˜ìŒ 5ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬ (MVP)
        sample_files = pdf_files[:5]
        # sample_files = [self.pdf_dir / "01-ê²½ì˜ëŒ€í•™.pdf"]
        
        for i, pdf_file in enumerate(sample_files):
            try:
                if progress_callback:
                    progress_callback(f"ì²˜ë¦¬ ì¤‘: {pdf_file.name} ({i+1}/{len(sample_files)})")
                
                print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_file.name}")
                
                # PDF íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
                pdf_filename = pdf_file.stem
                
                # PDFë³„ í…ìŠ¤íŠ¸ í´ë” ìƒì„±
                pdf_text_dir = self.temp_texts_dir / pdf_filename
                pdf_text_dir.mkdir(exist_ok=True)
                
                # 1. PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
                image_paths = self.pdf_extractor.extract_images_from_pdf(
                    str(pdf_file),
                    str(self.temp_images_dir),
                    split_large_pages=True
                )
                
                print(f"  ğŸ“· ì¶”ì¶œëœ ì´ë¯¸ì§€: {len(image_paths)}ê°œ")
                
                check_memory = True # = self.check_memory_threshold()
                if check_memory:
                    print("âš ï¸ ë©”ëª¨ë¦¬ ì„ê³„ê°’ ì´ˆê³¼ - ê°•ì œ ì •ë¦¬ ë° ëŒ€ê¸°")
                    self.force_memory_cleanup()
                    import time
                    time.sleep(2)  # ë©”ëª¨ë¦¬ ì•ˆì •í™” ëŒ€ê¸°
                
                # 2. OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì €ì¥
                pdf_texts = []
                for page_idx, img_path in enumerate(image_paths):
                    try:
                        text = self.ocr.extract_text(img_path)
                        if text.strip():  # ë¹ˆ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
                            clean_text = text.strip()
                            pdf_texts.append(clean_text)                                
                    except Exception as e:
                        print(f"    âš ï¸ OCR ì‹¤íŒ¨: {img_path} - {e}")
                        continue
                    
                # 3. í…ìŠ¤íŠ¸ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜í•˜ê³  ì €ì¥
                pdf_documents = []
                for j, text in enumerate(pdf_texts):
                    if len(text) > 50:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                        # í…ìŠ¤íŠ¸ ë¶„í• 
                        chunks = self.text_splitter.split_text(text)
                    else:
                        chunks = [text]    
                        
                    for k, chunk in enumerate(chunks):
                        doc = Document(
                            page_content=chunk,
                            metadata={
                                "source": pdf_file.name,
                                "page": j + 1,
                                "chunk": k + 1,
                                "processed_at": datetime.now().isoformat()
                            }
                        )
                        pdf_documents.append(doc)
                
                # 4. PDFë³„ Document ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
                if pdf_documents:
                    self._save_pdf_documents(pdf_filename, pdf_documents)
                    processed_pdfs.append(pdf_filename)
                    print(f"  âœ… ì™„ë£Œ: {pdf_file.name} - {len(pdf_documents)}ê°œ ë¬¸ì„œ ì €ì¥")
                else:
                    print(f"  âš ï¸ ì²˜ë¦¬í•  ë¬¸ì„œ ì—†ìŒ: {pdf_file.name}")
                    
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {pdf_file.name} - {e}")
                continue
            
        # Phase 1 ì •ë¦¬
        self.ocr.cleanup_ocr_model()
        
        if not processed_pdfs:
            print("âš ï¸ OCR ì²˜ë¦¬ëœ PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"\nğŸ“Š Phase 1 ì™„ë£Œ: {len(processed_pdfs)}ê°œ PDF ì²˜ë¦¬")
        for pdf_name in processed_pdfs:
            print(f"  âœ“ {pdf_name}")
        
        return processed_pdfs
    
    def _save_pdf_documents(self, pdf_filename: str, documents: List[Document]):
        """PDFë³„ Document ë¦¬ìŠ¤íŠ¸ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        documents_dir = self.temp_texts_dir / pdf_filename / "documents"
        documents_dir.mkdir(exist_ok=True)
        
        # Document ê°ì²´ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        doc_data = []
        for doc in documents:
            doc_data.append({
                "page_content": doc.page_content,
                "metadata": doc.metadata
            })
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        json_path = documents_dir / f"{pdf_filename}_documents.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(doc_data, f, ensure_ascii=False, indent=2)
        
        print(f"    ğŸ’¾ Document ì €ì¥: {json_path}")
    
    def _phase2_batch_vector_addition(self, processed_pdfs: List[str], progress_callback: Optional[Callable] = None) -> bool:
        """
        Phase 2: ì €ì¥ëœ Documentë“¤ì„ ë°°ì¹˜ë¡œ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
        
        Args:
            processed_pdfs: ì²˜ë¦¬ëœ PDF íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
            progress_callback: ì§„í–‰ìƒí™© ì½œë°± í•¨ìˆ˜
            
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        print("\nğŸ”„ Phase 2: ë²¡í„° ìŠ¤í† ì–´ ë°°ì¹˜ ì²˜ë¦¬")
        print("-" * 40)
        
        try:
            all_documents = []
            
            # ì €ì¥ëœ Document íŒŒì¼ë“¤ ë¡œë“œ
            for i, pdf_filename in enumerate(processed_pdfs):
                if progress_callback:
                    progress_callback(f"Document ë¡œë“œ ì¤‘: {pdf_filename} ({i+1}/{len(processed_pdfs)})")
                
                documents = self._load_pdf_documents(pdf_filename)
                if documents:
                    all_documents.extend(documents)
                    print(f"  ğŸ“„ {pdf_filename}: {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ")
            
            if not all_documents:
                print("âš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            print(f"\nğŸ“Š ì´ ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(all_documents)}ê°œ")
            
            if progress_callback:
                progress_callback(f"ë²¡í„° ì„ë² ë”© ìƒì„± ì¤‘... ({len(all_documents)}ê°œ ë¬¸ì„œ)")

            # VectorStoreManagerë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
            try:
                self.initialize_vector_manager()
                
                print(f"   ğŸ¤– VectorStoreManagerë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
                
                # 5. ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ìë™ ì €ì¥
                # Ensure vector_manager is initialized before use
                self.initialize_vector_manager()
                if self.vector_manager is not None:
                    self.vector_store = self.vector_manager.auto_save_after_creation(
                        documents=all_documents,
                        index_name=self.DEFAULT_INDEX_NAME,
                        model_key=self.DEFAULT_MODEL_KEY
                    )
                else:
                    raise RuntimeError("VectorStoreManager is not initialized.")
                
                print(f"   âœ… VectorStoreManagerë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
                
            except Exception as vector_manager_error:
                print(f"   âš ï¸ VectorStoreManager ì‹¤íŒ¨: {vector_manager_error}")                

            
            # 6. ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "created_at": datetime.now().isoformat(),
                "total_documents": len(all_documents),
                "processed_pdfs": processed_pdfs,
                "vector_db_path": str(self.vector_db_dir)
            }
            
            metadata_path = self.vector_db_dir / "metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            if progress_callback:
                progress_callback(f"ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ! (ë¬¸ì„œ {len(all_documents)}ê°œ)")
            
            print(f"âœ… Phase 2 ì™„ë£Œ: ë²¡í„° ìŠ¤í† ì–´ ë°°ì¹˜ ì²˜ë¦¬ ì„±ê³µ!")
            print(f"   ì €ì¥ ìœ„ì¹˜: {self.vector_db_dir}")
            print(f"   ì´ ë¬¸ì„œ: {len(all_documents)}ê°œ")
            
            return True
            
        except Exception as e:
            error_msg = f"Phase 2 ì‹¤íŒ¨: {e}"
            print(f"âŒ {error_msg}")
            if progress_callback:
                progress_callback(error_msg)
            return False
    
    def _load_pdf_documents(self, pdf_filename: str) -> List[Document]:
        """ì €ì¥ëœ PDF Document íŒŒì¼ì„ ë¡œë“œ"""
        try:
            json_path = self.temp_texts_dir / pdf_filename / "documents" / f"{pdf_filename}_documents.json"
            
            if not json_path.exists():
                print(f"âš ï¸ Document íŒŒì¼ ì—†ìŒ: {json_path}")
                return []
            
            with open(json_path, 'r', encoding='utf-8') as f:
                doc_data = json.load(f)
            
            # ë”•ì…”ë„ˆë¦¬ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
            documents = []
            for data in doc_data:
                doc = Document(
                    page_content=data["page_content"],
                    metadata=data["metadata"]
                )
                documents.append(doc)
            
            return documents
            
        except Exception as e:
            print(f"âŒ Document ë¡œë“œ ì‹¤íŒ¨ ({pdf_filename}): {e}")
            return []
    
    def initialize_vector_db(self, force_rebuild: bool = False, progress_callback: Optional[Callable] = None):
        """ë²¡í„° DB ì´ˆê¸°í™” - ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥í•œ ì´ˆê¸°í™” í•¨ìˆ˜"""
        try:
            if progress_callback:
                progress_callback("ë²¡í„° DB ì´ˆê¸°í™” ì‹œì‘...")
            
            print("ğŸ”„ ë²¡í„° DB ì´ˆê¸°í™” ì‹œì‘...")
            
            # ê¸°ì¡´ ë²¡í„° DB í™•ì¸
            if self.vector_store_exists() and not force_rebuild:
                if progress_callback:
                    progress_callback("ê¸°ì¡´ ë²¡í„° DB ë°œê²¬ - ê²€ì¦ ì¤‘...")
                
                print("ğŸ“ ê¸°ì¡´ ë²¡í„° DB ë°œê²¬ - ê²€ì¦ ì‹œë„...")
                try:
                    # VectorStoreManagerë¥¼ ë¨¼ì € ì‹œë„
                    self.initialize_vector_manager()
                    
                    if self.vector_manager is not None:
                        # VectorStoreManagerë¡œ ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
                        result, message = self.vector_manager.load_vector_store(self.DEFAULT_INDEX_NAME, self.DEFAULT_MODEL_KEY)
                        
                        if result:
                            self.vector_store = self.vector_manager.current_vector_store
                            
                            if progress_callback:
                                progress_callback("VectorStoreManager ë²¡í„° DB ê²€ì¦ ì™„ë£Œ!")
                            
                            print("VectorStoreManager ë²¡í„° DB ê²€ì¦ ì™„ë£Œ!")
                            return True, "VectorStoreManager ë²¡í„° DB ê²€ì¦ ì™„ë£Œ."
                        else:
                            print(f"     VectorStoreManager ë¡œë“œ ì‹¤íŒ¨: {message}")
                        
                    raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    if progress_callback:
                        progress_callback(f"ê¸°ì¡´ DB ê²€ì¦ ì‹¤íŒ¨ - ìƒˆë¡œ êµ¬ì¶•: {e}")
                    
                    print(f"âš ï¸ ê¸°ì¡´ DB ê²€ì¦ ì‹¤íŒ¨ - ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤: {e}")
                    force_rebuild = True
            
            # ìƒˆ ë²¡í„° DB êµ¬ì¶• ë˜ëŠ” ê°•ì œ ì¬êµ¬ì¶•
            if not self.vector_store_exists() or force_rebuild:
                if force_rebuild:
                    if progress_callback:
                        progress_callback("ê¸°ì¡´ ë²¡í„° DB ì‚­ì œ í›„ ìƒˆë¡œ êµ¬ì¶•...")
                    
                    print("ğŸ—‘ï¸ ê¸°ì¡´ ë²¡í„° DB ì‚­ì œ í›„ ìƒˆë¡œ êµ¬ì¶•...")
                    # VectorStoreManagerì˜ delete_saved_index ë©”ì„œë“œ ì‚¬ìš©
                    if self.vector_manager:
                        delete_result = self.vector_manager.delete_saved_index(self.DEFAULT_INDEX_NAME, self.DEFAULT_MODEL_KEY)
                        print(f"   {delete_result}")
                    else:
                        # vector_managerê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ìˆ˜ë™ ì‚­ì œ
                        import shutil
                        if self.vector_db_dir.exists():
                            shutil.rmtree(self.vector_db_dir)
                            self.vector_db_dir.mkdir(exist_ok=True)
                
                if progress_callback:
                    progress_callback("ìƒˆ ë²¡í„° DB êµ¬ì¶• ì‹œì‘...")
                
                print("ğŸ—ï¸ ìƒˆ ë²¡í„° DB êµ¬ì¶• ì‹œì‘...")
                self.build_vector_store(progress_callback)
                
                if progress_callback:
                    progress_callback("âœ… ìƒˆ ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ!")
                
                print("âœ… ìƒˆ ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ!")
                return True, "ìƒˆ ë²¡í„° DBë¥¼ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            error_msg = f"ë²¡í„° DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"
            print(f"âŒ {error_msg}")
            
            if progress_callback:
                progress_callback(f"âŒ {error_msg}")
            
            return False, error_msg
    
    def get_vector_store_info(self) -> Dict[str, Any]:
        """ë²¡í„° ìŠ¤í† ì–´ ì •ë³´ ë°˜í™˜"""
        info = {
            "exists": self.vector_store_exists(),
            "initialized": self.vector_store is not None,
            "vector_db_path": str(self.vector_db_dir)
        }
        
        # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
        metadata_path = self.vector_db_dir / "metadata.json"
        if metadata_path.exists():
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                info.update(metadata)
            except Exception as e:
                info["metadata_error"] = str(e)
        
        return info